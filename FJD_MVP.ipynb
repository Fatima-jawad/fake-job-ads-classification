{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62B_Wl4t7knh"
   },
   "source": [
    "# Fake Job Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcdjfH3o9ID1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcHmlxnh7knm",
    "outputId": "90224f7f-42a5-4a46-af4d-9802a527331e"
   },
   "outputs": [],
   "source": [
    "# Libraries used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder#, OneHotEncoder \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSaeFl30jKS4"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGprA0Mv7kno"
   },
   "outputs": [],
   "source": [
    "# We'll now upload the original preprocessed dataset\n",
    "df = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "yVtyW31q7knq",
    "outputId": "06bc9506-ae29-4eee-dd1c-d476adc2b68f"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCBIW6St7knt",
    "outputId": "8763957e-c75d-4141-fed4-cee848491233"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxek52J2qAwS",
    "outputId": "5f3a54b5-e861-4fd1-8f80-c4822ecf959b"
   },
   "outputs": [],
   "source": [
    "# We have to be carefull when selecting how to deal with Nans to avoid significant drop in minority data\n",
    "a = df[~df.description.isna()]\n",
    "b = df[~df.company_profile.isna()]\n",
    "c = df[~df.requirements.isna()]\n",
    "d = df[~df.benefits.isna()]\n",
    "e = df[~df.isna()]\n",
    "f = df[~df.salary_range.isna()]\n",
    "g = df[~df.telecommuting.isna()]\n",
    "h = df[~df.has_company_logo.isna()]\n",
    "i = df[~df.has_questions.isna()]\n",
    "j = df[~df.employment_type.isna()]\n",
    "k = df[~df.required_education.isna()]\n",
    "l = df[~df.required_experience.isna()]\n",
    "\n",
    "print('Original:           ',df.shape, df[df.fraudulent == 1].shape, df[df.fraudulent == 0].shape)\n",
    "print('Description:        ',a.shape, a[a.fraudulent == 1].shape, a[a.fraudulent == 0].shape)\n",
    "print('Company Profile:    ',b.shape, b[b.fraudulent == 1].shape, b[b.fraudulent == 0].shape)\n",
    "print('Requirements:       ',c.shape, c[c.fraudulent == 1].shape, c[c.fraudulent == 0].shape)\n",
    "print('Benefits:           ',d.shape, d[d.fraudulent == 1].shape, d[d.fraudulent == 0].shape)\n",
    "print('Salary Range:       ',f.shape, f[f.fraudulent == 1].shape, f[f.fraudulent == 0].shape)\n",
    "print('Telecommuting:      ',g.shape, g[g.fraudulent == 1].shape, g[g.fraudulent == 0].shape)\n",
    "print('Has Company Logo:   ',h.shape, h[h.fraudulent == 1].shape, h[h.fraudulent == 0].shape)\n",
    "print('Has Questions:      ',i.shape, i[i.fraudulent == 1].shape, i[i.fraudulent == 0].shape)\n",
    "print('Employment Type:    ',j.shape, j[j.fraudulent == 1].shape, j[j.fraudulent == 0].shape)\n",
    "print('Required Education: ',k.shape, k[k.fraudulent == 1].shape, k[k.fraudulent == 0].shape)\n",
    "print('Required Experience:',l.shape, l[l.fraudulent == 1].shape, l[l.fraudulent == 0].shape)\n",
    "print('All:                ',e.shape, e[e.fraudulent == 1].shape, e[e.fraudulent == 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y83ATRYA7knt"
   },
   "outputs": [],
   "source": [
    "# We'll fill all None values with 'Missing' \n",
    "df.fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZe0rKYk7knu"
   },
   "outputs": [],
   "source": [
    "# Here we are going to compine all texts data into one columns to ease the NLP process.\n",
    "df['texts'] = df['company_profile'] + '.. ' + df['description'] + '.. ' + df['requirements'] + '.. ' + df['benefits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "bd7rQXZm7kn0",
    "outputId": "fe3eb879-c079-4487-ccc4-71716e1dbc1b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll now drop all the columns that will not be needed.\n",
    "# We'll also drop the columns including the texts data that we combined before.\n",
    "df.drop(columns=['company_profile','description','requirements','benefits','location','title','industry','department','function','job_id'],inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_PBfwJu7kn1",
    "outputId": "9dc11a8d-b893-4b45-ac15-6da5cf43024a"
   },
   "outputs": [],
   "source": [
    "df['salary_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XQvrL8h7kn2",
    "outputId": "56d4425a-48cb-4266-9056-52f75218fa1e"
   },
   "outputs": [],
   "source": [
    "# We'll check if there was a salary range in the data and we'll take the difference of the range. If there is no salary range, a zero value will be assigned.\n",
    "sal_range = []\n",
    "for i in df.salary_range:\n",
    "    try: \n",
    "        sal_range.append(abs(int(i.split('-')[0]) - int(i.split('-')[1])))\n",
    "    except:\n",
    "        sal_range.append(0)\n",
    "df['salary_range'] = sal_range\n",
    "df['salary_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7sHLFAz7kn4",
    "outputId": "de4ee408-00d5-49ff-e18c-e602156ccb66"
   },
   "outputs": [],
   "source": [
    "# We'll check now if there is 'Missing' value in any of the columns\n",
    "print(df['telecommuting'].unique())\n",
    "print(df['has_company_logo'].unique())\n",
    "print(df['has_questions'].unique())\n",
    "print(df['employment_type'].unique())\n",
    "print(df['required_experience'].unique())\n",
    "print(df['required_education'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hin5y_Xd7kn8"
   },
   "outputs": [],
   "source": [
    "# We'll convert the 'Missing' data in the employment_type column and will assign it to the 'Other' type.\n",
    "df.employment_type.replace('Missing', 'Other', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cu9RT9Sa7kn9"
   },
   "outputs": [],
   "source": [
    "# We'll convert the 'Missing' data in the required_experience column and will assign it to 'Not Applicable'.\n",
    "df.required_experience.replace('Missing', 'Not Applicable', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVlkHSM67kn9"
   },
   "outputs": [],
   "source": [
    "# For the required_education column, we will join 'Vocational - Degree' and 'Vocational - HS Diploma' to 'Vocational'.\n",
    "# And we'll convert the 'Missing' data in the column to be 'Unspecified'.\n",
    "# We'll also convert 'Some High School Coursework' to 'High School or equivalent'.\n",
    "df.required_education.replace('Vocational - Degree', 'Vocational', inplace = True)\n",
    "df.required_education.replace('Vocational - HS Diploma', 'Vocational', inplace = True)\n",
    "df.required_education.replace('Missing', 'Unspecified', inplace = True)\n",
    "df.required_education.replace('Some High School Coursework', 'High School or equivalent', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psVAZkZm7kn-"
   },
   "outputs": [],
   "source": [
    "# We will use OrdinalEncoder to convert categorical features into numerical data that can be used in models.\n",
    "encoder = OrdinalEncoder()\n",
    "df[['employment_type',\n",
    "    'required_experience',\n",
    "    'required_education']] = encoder.fit_transform(df[['employment_type',\n",
    "                                                       'required_experience',\n",
    "                                                       'required_education']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNYSiepupHEw"
   },
   "source": [
    "## Dealing with Textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "kEh4ktMN7kn_",
    "outputId": "0d7e3f99-3300-4763-e9c2-6f7d14b50ef2"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbS3j8NT7koF"
   },
   "source": [
    "Now, lets drop all stop words and stem our words and get it tokenized. There is two ways to do that. The two main ways are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8fyZ2bc7koF"
   },
   "source": [
    "PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRlMhxUh7koF"
   },
   "source": [
    "The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "CH1FTq8b7koG",
    "outputId": "72f7f795-dd58-4b9a-82ec-daa7be79d336"
   },
   "outputs": [],
   "source": [
    "# We will first convert all texts to lowercase \n",
    "df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_q34toz7koG"
   },
   "outputs": [],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    meaningful_words = [w for w in token_words if not w in stops]\n",
    "    stemmed_list = [lancaster.stem(word) for word in meaningful_words]\n",
    "    joined_words = (' '.join(stemmed_list))\n",
    "    \n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ypDYVwy7koH"
   },
   "outputs": [],
   "source": [
    "df['texts'] = df['texts'].apply(identify_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExqAo5RZ7koI"
   },
   "outputs": [],
   "source": [
    "df.to_csv('Cleaned_data.csv') # cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h21Wcot7koI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cleaned_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiZp32h37koI",
    "outputId": "82c18127-e818-4b2c-c540-b4df3455319e"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "Sfztej2z7koI",
    "outputId": "f57cdf54-272f-438f-9677-8631b8da4025"
   },
   "outputs": [],
   "source": [
    "X_ = pd.concat([df[df.fraudulent ==1],df.iloc[np.random.choice(df[df.fraudulent == 0].index, size=(1,5000))[0]]])\n",
    "X_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjx8tbn7KsEF"
   },
   "outputs": [],
   "source": [
    "y = X_.fraudulent\n",
    "X_.drop('fraudulent', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVwTZ-g0AH2T"
   },
   "outputs": [],
   "source": [
    "# Now we will use the TfidfVectorizer().\n",
    "tfidf = TfidfVectorizer(decode_error='ignore')\n",
    "X = tfidf.fit_transform(df['texts'].values.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "JIV7q7PG7koJ",
    "outputId": "11e592b0-7392-4036-ede5-265f7546f382"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "w1wwy21PD3b4",
    "outputId": "0f3f68ff-fdd0-4548-9d14-f49d64222c17"
   },
   "outputs": [],
   "source": [
    "X[['salary_range',\n",
    "   'telecommuting', \n",
    "   'has_company_logo', \n",
    "   'has_questions',\n",
    "   'employment_type', \n",
    "   'required_experience', \n",
    "   'required_education',\n",
    "   'fraudulent']] =  X_[['salary_range', 'telecommuting', 'has_company_logo', \n",
    "                         'has_questions','employment_type', 'required_experience', \n",
    "                         'required_education','fraudulent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "Jtp-t55zD3PT",
    "outputId": "cf74c401-3d6c-4f6e-cb9e-49b38211418e"
   },
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEWozjcqFPgG"
   },
   "outputs": [],
   "source": [
    "X.to_csv('X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI2Zcm7Q2mG5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "nYXDtKLsayTX",
    "outputId": "3accd9c7-4040-4cc0-eb12-d0a3cdeb9f1f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niNN09i8ayP8",
    "outputId": "a0b5c6cb-9222-41e9-b82e-7090df24565c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlefpWbvayM5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYsBdGVPTDjI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "YYJr0rlOWbtW",
    "outputId": "45bc59e9-9e3e-4cf8-b520-74a821a29e31"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TalzR5IuT6zG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaMpeJNPc9rr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obZ_SiaO2k-k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIRmDBpA2k7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kt4GNUWZ2k4y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg7C9ddZ7koK"
   },
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56vE6Dtq7koK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhRRswNg7koL"
   },
   "source": [
    "## Create functions for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvRDIySY7koL"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "def log_reg_model(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    \n",
    "#     param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#                   'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#                   'C':range(1,1000)}\n",
    "    param_grid = {'C':range(1,1000)}\n",
    "    grid = GridSearchCV(model, param_grid, cv=10, scoring='recall')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best params:       \", grid.best_params_)\n",
    "    print(\"Best estimator:    \", grid.best_estimator_)\n",
    "    print(\"Best score:        \", grid.best_score_)\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    \n",
    "    print('Score:             ', final_model.score(X_train, y_train))\n",
    "    print('roc_auc_score:     ', roc_auc_score(y_test, y_pred))\n",
    "    print('precision_score:   ', precision_score(y_test, y_pred))\n",
    "    print('accuracy_score:    ', accuracy_score(y_test, y_pred))\n",
    "    print('recall_score:      ', recall_score(y_test, y_pred))\n",
    "    print('f1_score:          ', f1_score(y_test, y_pred))\n",
    "    \n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY6lMs3a7koL"
   },
   "outputs": [],
   "source": [
    "def knn_model(X_train, X_test, y_train, y_test):\n",
    "    model = KNeighborsClassifier()\n",
    "    k_range = list(range(1, 101))\n",
    "    weight_options = ['uniform', 'distance']\n",
    "    \n",
    "    param_grid = dict(n_neighbors=k_range, weights=weight_options)\n",
    "    \n",
    "    grid = GridSearchCV(model, param_grid, cv=10, scoring='recall')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best params: \", grid.best_params_)\n",
    "    print(\"Best estimator: \", grid.best_estimator_)\n",
    "    print(\"Best score: \", grid.best_score_)\n",
    "    \n",
    "    knn = grid.best_estimator_\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print('Score             :', knn.score(X_train, y_train))\n",
    "    print('roc_auc_score     :', roc_auc_score(y_test, y_pred))\n",
    "    print('precision_score   :', precision_score(y_test, y_pred))\n",
    "    print('accuracy_score    :', accuracy_score(y_test, y_pred))\n",
    "    print('recall_score      :', recall_score(y_test, y_pred))\n",
    "    print('f1_score          :', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1_Watqa7koL"
   },
   "outputs": [],
   "source": [
    "def D_T_C_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    dtc = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "    y_pred = dtc.predict(X_test)\n",
    "    \n",
    "    print('Score             :', dtc.score(X_train, y_train))\n",
    "    print('roc_auc_score     :', roc_auc_score(y_test, y_pred))\n",
    "    print('precision_score   :', precision_score(y_test, y_pred))\n",
    "    print('accuracy_score    :', accuracy_score(y_test, y_pred))\n",
    "    print('recall_score      :', recall_score(y_test, y_pred))\n",
    "    print('f1_score          :', f1_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snxVGpX87koM"
   },
   "outputs": [],
   "source": [
    "def R_F_C_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=100).fit(X_train,y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    \n",
    "    print('Score             :', rfc.score(X_train, y_train))\n",
    "    print('roc_auc_score     :', roc_auc_score(y_test, y_pred))\n",
    "    print('precision_score   :', precision_score(y_test, y_pred))\n",
    "    print('accuracy_score    :', accuracy_score(y_test, y_pred))\n",
    "    print('recall_score      :', recall_score(y_test, y_pred))\n",
    "    print('f1_score          :', f1_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oodc-Fdw7koM"
   },
   "source": [
    "## Fitting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "62Knb2u67koM",
    "outputId": "aa95bbb6-dfba-4213-9617-170da07ed1b4"
   },
   "outputs": [],
   "source": [
    "X = df.drop('fraudulent')\n",
    "y = df.fraudulent\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVi79TFS7koM",
    "outputId": "f04b22aa-bb11-4d7c-f452-6d5a3779ed97"
   },
   "outputs": [],
   "source": [
    "log_reg_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmEIFrdO7koM"
   },
   "outputs": [],
   "source": [
    "knn_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZOPpe6s7koN",
    "outputId": "c6c3f30f-9615-4584-954b-78edf603175f"
   },
   "outputs": [],
   "source": [
    "D_T_C_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBqTDLSY7koN",
    "outputId": "c036c704-991f-43a5-b6ef-a14d33d64f82"
   },
   "outputs": [],
   "source": [
    "R_F_C_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyTGd6DO7koR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "LcdpgIxu7koN",
    "evDYpkHa7koO"
   ],
   "name": "FJD_MVP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
